{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbooks to JSON\n",
    "\n",
    "#### Author\n",
    "\n",
    "Tijmen de Haan <tijmen@post.kek.jp>\n",
    "\n",
    "#### History\n",
    "\n",
    " - 2023 Nov 25 started coding\n",
    "\n",
    "#### Description\n",
    "\n",
    "I have extracted 250MB of text data from public-domain astro-related textbooks. Unfortunately, some of the books are misformatted, irrelevant, or low-quality. This notebook serves to turn the large amount of raw data into medium- to high-quality data chunks ready to be tokenized and fine-tuned on.\n",
    "\n",
    "### Plan\n",
    "\n",
    "#### Step 1: loss function-based filter\n",
    "\n",
    "I already fine tuned zephyr-7b-beta on papers from the arxiv and a physics QA dataset from huggingface. This fine-tuned model called \"zephyr-7b-beta_cosmosage_v1\" took 11 hours of compute on one A6000 GPU. \n",
    "\n",
    "Let's try using this fine-tuned model to evaluate the quality of these textbooks individually. We'll take a few small \"biopsies\" from each book and calculate the loss function of the fine-tuned model. Then we'll plot up these loss functions by book. The books that are very poorly predicted are likely junk data and can be thrown out. \n",
    "\n",
    "#### Step 2: manual rejection of certain textbooks\n",
    "\n",
    "My dataset contains some weird books! I believe I saw one about UFOs. This is not intended to be in the expertise of cosmosage, so these need to be manually rejected.\n",
    "\n",
    "#### Step 3: trimming\n",
    "\n",
    "For each of the remaining books, we'll want to trim off some initial part corresponding to the title page and such, and some final part that may consist of references or other irrelevant stuff. We will use the incidence of special characters (characters other than letters, punctuation, and whitespace) to decide where to start and stop the trim.\n",
    "\n",
    "#### Step 4 (optional): chunking\n",
    "\n",
    "We'll evaluate if it's at all possible to chunk by paragraph. If this appears difficult, we'll ignore it, as the Tokenizer will chunk by token count, anyway.\n",
    "\n",
    "#### Step 5: save prepared dataset as JSON\n",
    "\n",
    "Save as \"list\"-type JSON file, ready to be loaded into a TextDataset. The current plan is to train for one epoch on this dataset, then train one additional epoch on the other smaller higher-quality dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"zephyr-7b-beta_cosmosage_v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "model = (\n",
    "    AutoModelForCausalLM.from_pretrained(fine_tuned_model_path)\n",
    "    .to(\"cuda\")\n",
    "    .to(dtype=torch.bfloat16)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text_chunks(file_path, chunk_size=512, num_samples=16):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    tokens = tokenizer.encode(text)\n",
    "    max_start = max(0, len(tokens) - chunk_size)\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        start = random.randint(0, max_start)\n",
    "        end = start + chunk_size\n",
    "        sample = tokens[start:end]\n",
    "        samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "textbooks_dir = \"datasets/textbooks_extracted/\"\n",
    "textbook_files = os.listdir(textbooks_dir)\n",
    "\n",
    "# Filter out textbook files smaller than 1 kB\n",
    "textbook_files = [file for file in textbook_files if os.path.getsize(os.path.join(textbooks_dir, file)) >= 1024]\n",
    "\n",
    "# Now, sort the remaining textbook_files by file size\n",
    "textbook_files = sorted(textbook_files, key=lambda file: os.path.getsize(os.path.join(textbooks_dir, file)))\n",
    "\n",
    "resume = True\n",
    "if not resume:\n",
    "    loss = {}  # Mapping from book index to its loss, do this the first time\n",
    "else:\n",
    "    loss = torch.load(\"loss.pt\")  # Load loss dict from disk to resume\n",
    "\n",
    "# some books may have been deleted manually, remove those from the saved loss dict\n",
    "for file in list(loss.keys()):\n",
    "    if file not in textbook_files:\n",
    "        del loss[file]\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "for file in textbook_files:  # Just try a few books for now\n",
    "\n",
    "    # save loss dict to disk in case of crashes\n",
    "    torch.save(loss, \"loss.pt\")\n",
    "\n",
    "    if file in loss:\n",
    "        continue  # Skip books we've already evaluated\n",
    "    print(f\"Collecting samples from {file}.\")\n",
    "    file_path = os.path.join(textbooks_dir, file)\n",
    "    samples = sample_text_chunks(file_path, num_samples=batch_size)\n",
    "    \n",
    "    print(\"Converting all samples into a DataLoader.\")\n",
    "    padded_samples = pad_sequence([torch.tensor(sample) for sample in samples], batch_first=True, padding_value=0)\n",
    "    all_samples_tensor = padded_samples.to(\"cuda\")\n",
    "    dataset = TensorDataset(all_samples_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)  # Adjust batch size as needed\n",
    "\n",
    "    print(\"Evaluating model on samples.\")\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        this_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs = batch[0]\n",
    "            outputs = model(inputs, labels=inputs)\n",
    "            this_loss += outputs.loss.item()\n",
    "        loss[file] = this_loss\n",
    "    print(f\"Loss for {file}: {loss[file]}\")\n",
    "\n",
    "    \n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9494b9d",
   "metadata": {},
   "source": [
    "### Step 2: Manual Rejection of Certain Textbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f62b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043a3fa",
   "metadata": {},
   "source": [
    "### Step 3: Trimming Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd569d2",
   "metadata": {},
   "source": [
    "### Step 4 (Optional): Chunking by Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c84fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6efc4b",
   "metadata": {},
   "source": [
    "### Step 5: Saving the Prepared Dataset as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c848a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "textbooks_path = glob.glob('datasets/textbooks_extracted/*.txt')\n",
    "textbooks = []\n",
    "for textbook_path in textbooks_path:\n",
    "    with open(textbook_path, 'r', encoding='utf-8') as textbook_file:\n",
    "        textbook = textbook_file.read()\n",
    "    textbooks.append(textbook)\n",
    "\n",
    "# Saving the data as JSON\n",
    "with open(\"datasets/textbooks_clean.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(textbooks, json_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
