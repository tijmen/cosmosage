{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textbooks to JSON\n",
    "\n",
    "#### Author\n",
    "\n",
    "Tijmen de Haan <tijmen@post.kek.jp>\n",
    "\n",
    "#### History\n",
    "\n",
    " - 2023 Nov 25 started coding\n",
    " - 2023 Nov 29 changed from a model loss-based book selection to one based on special character counts\n",
    "\n",
    "#### Description\n",
    "\n",
    "I have extracted text data from public-domain astro-related textbooks. Unfortunately, some of the books are misformatted, irrelevant, or low-quality. This notebook serves to turn the large amount of raw data into medium-quality data chunks ready to be tokenized and pretrained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Filter books with special character statistics\n",
    "\n",
    "We'll use the rate of special characters to evaluate the quality of these textbooks. The books that are out of distribution get thrown out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def calculate_percentages(textbook):\n",
    "    space_percentage = 100 * textbook.count(' ') / len(textbook)\n",
    "    newline_percentage = 100 * textbook.count('\\n') / len(textbook)\n",
    "    backslash_percentage = 100 * textbook.count('\\\\') / len(textbook)\n",
    "    return space_percentage, newline_percentage, backslash_percentage\n",
    "\n",
    "def histogram_percentages(textbooks):\n",
    "    space_percentages = []\n",
    "    newline_percentages = []\n",
    "    backslash_percentages = []\n",
    "\n",
    "    for book in textbooks:\n",
    "        space_perc, newline_perc, backslash_perc = calculate_percentages(book)\n",
    "        space_percentages.append(space_perc)\n",
    "        newline_percentages.append(newline_perc)\n",
    "        backslash_percentages.append(backslash_perc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(space_percentages, bins=50, range=(0, 30), alpha=0.5, label='Spaces')\n",
    "    plt.hist(newline_percentages, bins=50, range=(0, 30), alpha=0.5, label='Newlines')\n",
    "    plt.hist(backslash_percentages, bins=50, range=(0, 30), alpha=0.5, label='Backslashes')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.ylabel('Number of Textbooks')\n",
    "    plt.title('Character Distribution in Textbooks')\n",
    "    plt.show()\n",
    "\n",
    "def filter_textbooks(textbooks, space_bounds, newline_bounds, backslash_bounds):\n",
    "    filtered_books = []\n",
    "    for book in textbooks:\n",
    "        space_perc, newline_perc, backslash_perc = calculate_percentages(book)\n",
    "        if space_bounds[0] <= space_perc <= space_bounds[1] and \\\n",
    "           newline_bounds[0] <= newline_perc <= newline_bounds[1] and \\\n",
    "           backslash_bounds[0] <= backslash_perc <= backslash_bounds[1]:\n",
    "            filtered_books.append(book)\n",
    "    return filtered_books\n",
    "\n",
    "def save_data(filtered_books, output_file_path):\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(filtered_books, file, indent=4)\n",
    "\n",
    "file_path = 'datasets/textbooks_clean.json'\n",
    "output_file_path = 'datasets/textbooks_clean2.json'\n",
    "\n",
    "textbooks = load_data(file_path)\n",
    "histogram_percentages(textbooks)\n",
    "\n",
    "# Adjusted based on visual inspection of the histograms\n",
    "space_bounds = (13.0, 19.5)  # min and max percentage of spaces\n",
    "newline_bounds = (1.0, 6.0)  # min and max percentage of newlines\n",
    "backslash_bounds = (0.0, 1.0)  # min and max percentage of backslashes\n",
    "\n",
    "filtered_books = filter_textbooks(textbooks, space_bounds, newline_bounds, backslash_bounds)\n",
    "print(f\"After filtering, {len(filtered_books)} out of {len(textbooks)} books remain.\")\n",
    "save_data(filtered_books, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize and save\n",
    "\n",
    "Textbooks are very long. Ideally we'd do something like in-context learning from parts and summarize with an advanced model like GPT-4. Since I don't have any money for this right now, I'll just chunk the textbooks into fixed-length tokenized samples. This will be used to pretrain.\n",
    "\n",
    "We use pickle for now to save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def load_tokenizer():\n",
    "    return PreTrainedTokenizerFast(tokenizer_file=\"Yi-6B/tokenizer.json\")\n",
    "\n",
    "def tokenize_book(book, tokenizer_working_size=10000, chunk_size=512):\n",
    "    '''\n",
    "    Method to chunk a book. We want to cut the book into `chunk_size` token chunks,\n",
    "    but we don't a priori know how much of the book is `chunk_size` tokens long. So we first\n",
    "    tokenize the whole book, then chunk it into `chunk_size` token chunks.\n",
    "    '''\n",
    "    tokenizer = load_tokenizer()  # Load separate tokenizer in each process for multiprocessing\n",
    "    pad_token_id = tokenizer.convert_tokens_to_ids('<unk>')  # Use <unk> as the pad token\n",
    "\n",
    "    # First tokenize the whole book. This has to happen in pieces \"tokenizer_working_size\" long\n",
    "    tokenized_book = []\n",
    "    for i in range(0, len(book), tokenizer_working_size):\n",
    "        part = book[i:i+tokenizer_working_size]\n",
    "        tokens = tokenizer.encode(part, add_special_tokens=False)\n",
    "        tokenized_book.extend(tokens)\n",
    "\n",
    "    # Now chunk the whole tokenized book into chunks of size \"chunk_size\"\n",
    "    # and manually pad each chunk to ensure consistent length\n",
    "    chunked_book = []\n",
    "    for i in range(0, len(tokenized_book), chunk_size):\n",
    "        chunk = tokenized_book[i:i+chunk_size]\n",
    "        padded_chunk = chunk + [pad_token_id] * (chunk_size - len(chunk))  # Pad the chunk\n",
    "        chunked_book.append(padded_chunk)\n",
    "\n",
    "    return chunked_book\n",
    "\n",
    "file_path = 'datasets/textbooks_clean2.json'\n",
    "textbooks = load_data(file_path)\n",
    "\n",
    "print(f\"Starting tokenization of {len(textbooks)} textbooks using multiprocessing...\")\n",
    "\n",
    "# Using Pool to utilize all available CPUs\n",
    "with multiprocessing.Pool() as pool:\n",
    "    chunked_books = pool.map(tokenize_book, textbooks)\n",
    "\n",
    "print(\"Flattening and shuffling...\")\n",
    "\n",
    "flat_chunks = []\n",
    "for chunked_book in chunked_books:\n",
    "    flat_chunks.extend(chunked_book)\n",
    "random.shuffle(flat_chunks)\n",
    "\n",
    "\n",
    "# Save tokenized data as pkl\n",
    "print(\"Saving pickle...\")\n",
    "import pickle\n",
    "with open(\"Yi-6B_textbooks_v1/tokenized_dataset.pkl\", \"wb\") as file:\n",
    "    pickle.dump(flat_chunks, file)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - One idea is to not pad in the tokenizer but rather pad on the fly in the __getitem__ method of the dataset\n",
    "```\n",
    "def __getitem__(self, idx):\n",
    "    token_ids = self.data[idx]\n",
    "    encoded_dict = self.tokenizer.prepare_for_model(\n",
    "        token_ids,\n",
    "        max_length=self.max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encoded_dict\n",
    "```\n",
    "\n",
    " - Books tend to have crap at the start and the end. We could cut this out, potentially by using the model itself.\n",
    "\n",
    " - Here's some debugging code\n",
    " ```\n",
    "# logging.info(f\"memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "# logging.info(input_ids.shape)\n",
    "# logging.info(input_ids)\n",
    "# logging.info(input_ids.dtype)\n",
    "\n",
    "# output_512 = model(input_ids[:, :512], labels=input_ids[:, :512])\n",
    "# logging.info(f\"memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "# output_1024 = model(input_ids[:, :1024], labels=input_ids[:, :1024])\n",
    "# logging.info(f\"memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "# output_2048 = model(input_ids[:, :2048], labels=input_ids[:, :2048])\n",
    "# logging.info(f\"memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "# output_4096 = model(input_ids[:, :4096], labels=input_ids[:, :4096])\n",
    "# logging.info(f\"memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
