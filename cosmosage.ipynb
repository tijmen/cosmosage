{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosmosage\n",
    "\n",
    "See README for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import scrape_arxiv\n",
    "import tex_to_json\n",
    "import analyze_asl_dict\n",
    "import extract_textbooks\n",
    "import fine_tune\n",
    "import random\n",
    "\n",
    "tex_files_path = \"datasets/tex_files/\"\n",
    "json_file_path = \"datasets/arxiv_tex.json\"\n",
    "cleaned_json_file_path = \"datasets/combined_training_set.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract arXiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique arXiv numbers from the asl database\n",
    "db_path = \"datasets/dict_20231123.db\"\n",
    "arxiv_id_asl_tagged = analyze_asl_dict.extract_unique_arxiv_numbers(db_path)\n",
    "\n",
    "# also extract all of my papers\n",
    "search_params = {\"search_query\": \"au:de_Haan_T\", \"searchtype\": \"author\"}\n",
    "arxiv_id_tdh = scrape_arxiv.get_arxiv_ids(search_params)\n",
    "\n",
    "# also extract the papers with \"cosmic microwave background\" in the abstract\n",
    "search_params = {\"search_query\": \"abs:\\\"cosmic microwave background\\\"\"}\n",
    "arxiv_id_cmb = scrape_arxiv.get_arxiv_ids(search_params)\n",
    "\n",
    "# more arxiv papers recommended for me by asl\n",
    "arxiv_id_asl_rec = scrape_arxiv.other_arxiv_recommendation_ids()\n",
    "\n",
    "# join all of these arxiv ids and remove duplicates\n",
    "arxiv_ids = arxiv_id_asl_tagged + arxiv_id_tdh + arxiv_id_cmb + arxiv_id_asl_rec\n",
    "arxiv_ids = list(set(arxiv_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download the .tex files from arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sequential version (one thread):\n",
    "# scrape_arxiv.extract_tex(arxiv_ids, tex_files_path)\n",
    "\n",
    "# multithreaded version:\n",
    "from multiprocessing import Pool\n",
    "def download_papers(arxiv_id_list):\n",
    "    scrape_arxiv.extract_tex(arxiv_id_list, tex_files_path)\n",
    "n_processes = 12\n",
    "random.shuffle(arxiv_ids)\n",
    "arxiv_id_split = [arxiv_ids[i::n_processes] for i in range(n_processes)]\n",
    "with Pool(n_processes) as p:\n",
    "    p.map(download_papers, arxiv_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse the downloaded .tex files and save to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # method using pydetex\n",
    "# parsed_tex_files = tex_to_json.parse_tex_files(tex_files_path)\n",
    "# tex_to_json.save_to_json(parsed_tex_files, json_file_path)\n",
    "\n",
    "# method using command line detex \n",
    "tex_to_json.detex_files(\"datasets/tex_files/\")\n",
    "# manual regular expressions to clean up .detex and save to a single JSONL file\n",
    "tex_to_json.detex_to_jsonl(\"datasets/tex_files/\", \"datasets/arxiv_tex.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract textbooks and create another JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_textbooks.textbooks_to_jsonl(\"datasets/textbooks.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two options. We can either keep control of the training loop. To do this uncomment and run the following code. The other option is to train on the JSONL files with the `axolotl` package. The advantage of this is that it comes with a lot of bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL : manually collect training data, tokenize, and run training loop \n",
    "\n",
    "# JSON method for collating data\n",
    "# # clean arxiv json data a little more and include multiple copies\n",
    "# num_copies_arxiv = 4\n",
    "# json_data = tex_to_json.load_from_json(json_file_path)\n",
    "# cleaned_data = []\n",
    "# for _ in range(num_copies_arxiv):\n",
    "#     for paper, data_list in json_data.items():\n",
    "#         # remove any sequences enclosed in square brackets (e.g. [1])\n",
    "#         cleaned_data.extend([re.sub(r\"\\[[^\\]]*\\]\", \"\", data) for data in data_list])\n",
    "\n",
    "# # add physics Q&A data\n",
    "# physics_questions = tex_to_json.load_from_json(\"datasets/physics_clean.json\")\n",
    "# cleaned_data.extend(physics_questions)\n",
    "# tex_to_json.save_to_json(cleaned_data, cleaned_json_file_path)\n",
    "\n",
    "# Train the model\n",
    "# ALTERNATIVE: train using axolotl and its config.yml\n",
    "\n",
    "# fine_tune.fine_tune(\n",
    "#     pretrained_model_file_path=\"zephyr-7b-beta\",\n",
    "#     training_data=cleaned_json_file_path,\n",
    "#     lr=5e-5,\n",
    "#     gradient_clip=1.0,\n",
    "#     num_epochs=1,\n",
    "#     out_dir=\"zephyr-7b-beta_cosmosage_v1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss during training\n",
    "import plot_tf_log\n",
    "log = plot_tf_log.most_recent_log(\"mistral_cosmosage_v3\")\n",
    "plot_tf_log.plot_loss([log], logsmooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"/home/tijmen/cosmosage/models/mistral_cosmosage_v4/relora_out/\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def ask_cosmosage(question):\n",
    "    prompt = f\"\"\"<s>SYSTEM: You are an expert cosmology assistant, able to answer questions on the cosmic microwave background, galaxy formation, large scale structure, theoretical cosmology, inflation, big bang nucleosynthesis, cosmology instrumentation, and other related topics. Please assume the user is fluent in scientific terminology and do not insult his or her intelligence. Elaborate where possible to give a complete answer along with its context. If you do not know, say you do not know. Give one answer after reading the two provided examples.\n",
    "## EXAMPLE 1\n",
    "USER: What is the ISW effect?\n",
    "ASSISTANT: The Integrated Sachs-Wolfe effect describes the fact that cosmic microwave background photons gain energy as they fall into potential wells and lose it again as they climb back out. The origin of the effect is gravitational redshift. For static potential wells, there is no net effect. However, if the potentials grow or decay, there is a net effect, known as the Integrated Sachs-Wolfe effect. In a matter-dominated universe, linear potentials stay constant. The late-time ISW effect refers to the fact that at late times, dark energy causes a deviation from matter domination and linear potentials decay. This causes an effect in the cosmic microwave background which has been measured in cross-correlation with large-scale structure. The early-time ISW refers to the fact that right after recombination, radiation still contributed a non-negligible amount of the energy density of the universe, causing potentials to grow. This causes the photons to gain energy.\n",
    "## EXAMPLE 2\n",
    "USER: What are the definitions of galaxy cluster mass (M200m, M500c, etc.)?\n",
    "ASSISTANT: Calculating a galaxy cluster's mass is like calculating the population of a sprawling city; as you get further out from city center, the suburbs never end. Eventually you have to stop counting and just say that from there on the people belong to a different town.\\nIn cluster masses, we integrate from the cluster center out to some radius rΔ where Δ might be 200, 500, or some other number. The definition of this radius is that the density of the enclosed mass is Δ times the average density of the universe at that redshift. The associated mass is MΔ.\\nOne more detail is that 'average density' can be defined in two ways. You can either include the energy density of dark energy - in which case you are comparing to the critical density at that redshift - or you can use the matter density only. This choice results in the c or m subscript (stands for critical or mean). Note that the critical density is higher than the mean density, so it gives an earlier cutoff in the integral. Therefore 'MΔc' masses are smaller than 'MΔm' ones.\n",
    "## QUESTION\n",
    "USER: {question}\n",
    "ASSISTANT:\"\"\"\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using the fine-tuned model as an assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_cmb = ask_cosmosage(\"What is the temperature of the CMB?\") # easy question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_cmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dan = ask_cosmosage(\"What is Digital Active Nulling?\") # see if it's read the arxiv paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_isw = ask_cosmosage(\"Explain the ISW effect.\")  # hard question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_isw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear VRAM when not using it\n",
    "del pipe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
