{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosmosage\n",
    "\n",
    "See README for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import scrape_arxiv\n",
    "import analyze_asl_dict\n",
    "import extract_textbooks\n",
    "import glob\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# tex_files_path = \"datasets/tex_files/\"\n",
    "# json_file_path = \"datasets/arxiv_tex.json\"\n",
    "# cleaned_json_file_path = \"datasets/combined_training_set.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract arXiv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_file = \"datasets/arxiv_ids_cache.pkl\"\n",
    "\n",
    "# Check if the cache file exists\n",
    "if os.path.exists(cache_file):\n",
    "    # Load the cached data\n",
    "    with open(cache_file, \"rb\") as f:\n",
    "        arxiv_ids = pickle.load(f)\n",
    "else:\n",
    "    # unique arXiv numbers from the asl database\n",
    "    db_path = \"datasets/dict_20231123.db\"\n",
    "    arxiv_id_asl_tagged = analyze_asl_dict.extract_unique_arxiv_numbers(db_path)\n",
    "\n",
    "    # also extract all of my papers\n",
    "    search_params = {\"search_query\": \"au:de_Haan_T\", \"searchtype\": \"author\"}\n",
    "    arxiv_id_tdh = scrape_arxiv.get_arxiv_ids(search_params)\n",
    "\n",
    "    # also extract the papers with \"cosmic microwave background\" in the abstract\n",
    "    search_params = {\"search_query\": \"abs:\\\"cosmic microwave background\\\"\"}\n",
    "    arxiv_id_cmb = scrape_arxiv.get_arxiv_ids(search_params)\n",
    "\n",
    "    # more arxiv papers recommended for me by asl\n",
    "    arxiv_id_asl_rec = scrape_arxiv.other_arxiv_recommendation_ids()\n",
    "\n",
    "    # join all of these arxiv ids and remove duplicates\n",
    "    arxiv_ids = arxiv_id_asl_tagged + arxiv_id_tdh + arxiv_id_cmb + arxiv_id_asl_rec\n",
    "    arxiv_ids = list(set(arxiv_ids))\n",
    "\n",
    "    # Save the data to the cache file\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(arxiv_ids, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique arXiv numbers from the asl database\n",
    "db_path = \"datasets/dict_20231123.db\"\n",
    "arxiv_id_asl_tagged = analyze_asl_dict.extract_unique_arxiv_numbers(db_path)\n",
    "\n",
    "# also extract all of my papers\n",
    "search_params = {\"search_query\": \"au:de_Haan_T\", \"searchtype\": \"author\"}\n",
    "arxiv_id_tdh = scrape_arxiv.get_arxiv_ids(search_params)\n",
    "\n",
    "# also extract the papers with \"cosmic microwave background\" in the abstract\n",
    "search_params = {\"search_query\": \"abs:\\\"cosmic microwave background\\\"\"}\n",
    "arxiv_id_cmb = scrape_arxiv.get_arxiv_ids(search_params)\n",
    "\n",
    "# more arxiv papers recommended for me by asl\n",
    "arxiv_id_asl_rec = scrape_arxiv.other_arxiv_recommendation_ids()\n",
    "\n",
    "# join all of these arxiv ids and remove duplicates\n",
    "arxiv_ids = arxiv_id_asl_tagged + arxiv_id_tdh + arxiv_id_cmb + arxiv_id_asl_rec\n",
    "arxiv_ids = list(set(arxiv_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Synthetic data generation\n",
    "\n",
    "Here, we generate synthetic data using the following:\n",
    " - instruction-tuned model to generate the QA pairs\n",
    " - VLLM server to load the model once and provide good throughput\n",
    " - langchain to handle \n",
    "   - gathering of papers\n",
    "   - extracting from PDFs\n",
    "   - chunking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arxiv_id(arxiv_id):\n",
    "    try:\n",
    "        paper = scrape_arxiv.arxiv_paper(arxiv_id)\n",
    "        paper.generate_summary()\n",
    "        paper.generate_qa_pairs()\n",
    "        paper.save_dataset_jsonl()\n",
    "    except Exception as e:\n",
    "        # Log the exception and arxiv_id\n",
    "        print(f\"Error processing {arxiv_id}: {e}\")\n",
    "\n",
    "# Create a pool of workers\n",
    "pool = multiprocessing.Pool()\n",
    "\n",
    "# Map the process_arxiv_id function to each arxiv_id in parallel\n",
    "for arxiv_id in arxiv_ids:\n",
    "    if not os.path.exists(f\"datasets/arxiv_qa/{arxiv_id}.jsonl\"):\n",
    "        pool.apply_async(process_arxiv_id, args=(arxiv_id,))\n",
    "\n",
    "# Close the pool of workers\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code can take quite a while to run, so it is also available in script form at\n",
    "`run_generate_synth.py` which will run inside e.g. a screen session.\n",
    "\n",
    "A logger can be set up with `log_generate_synth.sh` to track the progress. The following code will plot the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and process log data\n",
    "def read_and_process_log(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_contents = file.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in log_contents:\n",
    "        match = re.search(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) - Folder Size: (\\d+(?:\\.\\d+)?)M - File Count: (\\d+)', line)\n",
    "        if match:\n",
    "            date_time = datetime.strptime(match.group(1), '%Y-%m-%d %H:%M:%S')\n",
    "            folder_size = float(match.group(2))\n",
    "            file_count = int(match.group(3))\n",
    "            data.append({'DateTime': date_time, 'FolderSizeMB': folder_size, 'FileCount': file_count})\n",
    "        else:\n",
    "            print(f'Error: Could not parse line: {line}')\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to plot file count and folder size\n",
    "def plot_file_count_and_folder_size(df):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plotting file count\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Date and Time')\n",
    "    ax1.set_ylabel('File Count', color=color)\n",
    "    ax1.plot(df['DateTime'], df['FileCount'], color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Plotting folder size with a second y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Folder Size (MB)', color=color)\n",
    "    ax2.plot(df['DateTime'], df['FolderSizeMB'], color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('File Count and Folder Size Over Time')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# File path to your log data\n",
    "log_file_path = 'datasets/arxiv_qa2/generate_synth.log'\n",
    "\n",
    "# Reading and processing the log data\n",
    "df = read_and_process_log(log_file_path)\n",
    "\n",
    "# Plotting the data\n",
    "plot_file_count_and_folder_size(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Join and prepare the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have many JSONL files, one for each arXiv paper. Each one has an initial question which asks to summarize the whole paper. The subsequent questions are specific questions about the paper. When I used Mistral-7B-v0.2 to generate these questions, it did not include much context and some of the questions are impossible to answer unless you know what context they are being asked in. For that reason, let's provide the summary as context to the Q&A.\n",
    "\n",
    "Let's also collate the summaries into a single JSONL file that uses QA format, without context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_qa_jsonl_files = glob.glob(\"datasets/arxiv_qa/*.jsonl\")\n",
    "\n",
    "# collate the first line of each JSONL file into a single summariesfile\n",
    "# arxiv_qa_summarize_jsonl_file = \"datasets/arxiv_qa2_summarize.jsonl\"\n",
    "# with open(arxiv_qa_summarize_jsonl_file, \"w\") as f1:\n",
    "#     for arxiv_qa_jsonl_file in arxiv_qa_jsonl_files:\n",
    "#         if os.path.exists(arxiv_qa_jsonl_file) and os.path.getsize(arxiv_qa_jsonl_file) > 0:\n",
    "#             with open(arxiv_qa_jsonl_file, \"r\") as g:\n",
    "#                 first_line = g.readline()\n",
    "#                 f1.write(first_line + \"\\n\")\n",
    "                \n",
    "conversations = []\n",
    "for arxiv_qa_jsonl_file in arxiv_qa_jsonl_files:\n",
    "    if os.path.exists(arxiv_qa_jsonl_file) and os.path.getsize(arxiv_qa_jsonl_file) > 10000:\n",
    "        with open(arxiv_qa_jsonl_file, \"r\") as g:\n",
    "            first_line = g.readline()\n",
    "            first_line_json = json.loads(first_line)\n",
    "            summary = first_line_json[\"answer\"]\n",
    "            if not isinstance(summary, str):\n",
    "                summary = str(summary)\n",
    "            if len(summary)>4000:\n",
    "                print(f\"Summary too big for {arxiv_qa_jsonl_file}\")\n",
    "                # cut it down to one paragraph\n",
    "                summary = summary.split(\"\\n\\n\")[0]\n",
    "            rest_of_lines = g.readlines()\n",
    "            conversation = [{\"from\": \"system\", \"value\": f\"{summary}. Below is a question from a USER. The ASSISTANT writes a response that appropriately answers the question. The USER may ask further questions which are answered by the ASSISTANT.\"}]\n",
    "            for i,line in enumerate(rest_of_lines):\n",
    "                #print(f\"Processing {arxiv_qa_jsonl_file} line {i} of {len(rest_of_lines)}\")\n",
    "                json_line = json.loads(line)\n",
    "                # check that this is a QA pair\n",
    "                if not \"question\" in json_line or not \"answer\" in json_line:\n",
    "                    print(f\"In file {arxiv_qa_jsonl_file}, line {i+1} of {len(rest_of_lines)} is invalid:\\n{json_line}\")\n",
    "                    continue\n",
    "                question = json_line[\"question\"]\n",
    "                answer = json_line[\"answer\"]\n",
    "                if not isinstance(question, str):\n",
    "                    question = str(question)\n",
    "                if not isinstance(answer, str):\n",
    "                    answer = str(answer)\n",
    "                if len(question)<10 or len(answer)<10:\n",
    "                    print(f\"In file {arxiv_qa_jsonl_file}, line {i+1} of {len(rest_of_lines)} is invalid:\\n{json_line}\")\n",
    "                    continue\n",
    "                conversation.append({\"from\": \"user\", \"value\": question})\n",
    "                conversation.append({\"from\": \"assistant\", \"value\": answer})\n",
    "                total_chars = sum(len(json.dumps(item)) for item in conversation)\n",
    "                if total_chars > 8500:\n",
    "                    if i==0:\n",
    "                        raise Exception(f\"Conversation too big, even with just the summary. {total_chars} characters.\")\n",
    "                    #print(f\"Conversation too big, ending it with {total_chars} characters.\")\n",
    "                    # too big, remove last QA pair and write to file\n",
    "                    conversations.append(json.dumps({\"conversations\": conversation[:-2]}) + \"\\n\")\n",
    "                    # delete all but the summary and last QA pair\n",
    "                    conversation = [conversation[0]] + conversation[-2:]\n",
    "            # write the remaining conversation to file\n",
    "            conversations.append(json.dumps({\"conversations\": conversation}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this sharegpt format conversation to file using datasets library\n",
    "\n",
    "# Write the conversations to a .jsonl file\n",
    "jsonl_file = 'datasets/arxiv_qa.jsonl'\n",
    "with open(jsonl_file, 'w') as file:\n",
    "    for conv in conversations:\n",
    "        file.write(conv)\n",
    "\n",
    "# # Write the conversations to a .json file\n",
    "# with open(arxiv_qa_sharegpt_file, 'w') as file:\n",
    "#     json.dump(conversations, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract textbooks and create another JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from extract_textbooks import TextBook\n",
    "textbooks = []\n",
    "for filepath in glob.glob(\"datasets/cosmology_textbooks/*.txt\"):\n",
    "    textbooks.append(TextBook(filepath))\n",
    "for textbook in textbooks:\n",
    "    textbook.generate_qa_pairs(multiprocess=True)\n",
    "    textbook.save_dataset_jsonl()\n",
    "    print(f\"Saved {textbook.author} to jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# collate all the JSONL files and shuffle them for good measure\n",
    "textbook_jsonl_files_in = glob.glob(\"datasets/cosmology_textbooks_qa/*/*.jsonl\")\n",
    "textbook_jsonl_file_out = \"datasets/cosmology_textbooks_qa.jsonl\"\n",
    "\n",
    "with open(textbook_jsonl_file_out, \"w\") as f:\n",
    "    all_lines = []\n",
    "    for textbook_jsonl_file in textbook_jsonl_files_in:\n",
    "        with open(textbook_jsonl_file, \"r\") as g:\n",
    "            all_lines.extend(g.readlines())\n",
    "    random.shuffle(all_lines)\n",
    "    f.writelines(all_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two options. We can either keep control of the training loop. To do this uncomment and run the following code. The other option is to train on the JSONL files with the `axolotl` package. The advantage of this is that it comes with a lot of bells and whistles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL : manually collect training data, tokenize, and run training loop \n",
    "\n",
    "# JSON method for collating data\n",
    "# # clean arxiv json data a little more and include multiple copies\n",
    "# num_copies_arxiv = 4\n",
    "# json_data = tex_to_json.load_from_json(json_file_path)\n",
    "# cleaned_data = []\n",
    "# for _ in range(num_copies_arxiv):\n",
    "#     for paper, data_list in json_data.items():\n",
    "#         # remove any sequences enclosed in square brackets (e.g. [1])\n",
    "#         cleaned_data.extend([re.sub(r\"\\[[^\\]]*\\]\", \"\", data) for data in data_list])\n",
    "\n",
    "# # add physics Q&A data\n",
    "# physics_questions = tex_to_json.load_from_json(\"datasets/physics_clean.json\")\n",
    "# cleaned_data.extend(physics_questions)\n",
    "# tex_to_json.save_to_json(cleaned_data, cleaned_json_file_path)\n",
    "\n",
    "# Train the model\n",
    "# ALTERNATIVE: train using axolotl and its config.yml\n",
    "\n",
    "# fine_tune.fine_tune(\n",
    "#     pretrained_model_file_path=\"zephyr-7b-beta\",\n",
    "#     training_data=cleaned_json_file_path,\n",
    "#     lr=5e-5,\n",
    "#     gradient_clip=1.0,\n",
    "#     num_epochs=1,\n",
    "#     out_dir=\"zephyr-7b-beta_cosmosage_v1\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run \n",
    "```accelerate launch -m axolotl.cli.train config.yml --prepare_ds_only --debug```\n",
    "to see examples of what data your model is being finetuned on. It is useful for knowing the exact prompt template to use during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize loss during training\n",
    "import plot_tf_log\n",
    "v16 = plot_tf_log.most_recent_log(\"mistral_cosmosage_v16\")\n",
    "v14 = plot_tf_log.most_recent_log(\"mistral_cosmosage_v14\")\n",
    "v15 = plot_tf_log.most_recent_log(\"mistral_cosmosage_v15\")\n",
    "plot_tf_log.plot_loss([v16], plot_type=\"detailed\", detailed_pts_per_eval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"/home/tijmen/cosmosage/models/mistral_cosmosage_v4/relora_out/\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "def ask_cosmosage(question):\n",
    "    prompt = f\"\"\"<s>SYSTEM: You are an expert cosmology assistant, able to answer questions on the cosmic microwave background, galaxy formation, large scale structure, theoretical cosmology, inflation, big bang nucleosynthesis, cosmology instrumentation, and other related topics. Please assume the user is fluent in scientific terminology and do not insult his or her intelligence. Elaborate where possible to give a complete answer along with its context. If you do not know, say you do not know. Give one answer after reading the two provided examples.\n",
    "## EXAMPLE 1\n",
    "USER: What is the ISW effect?\n",
    "ASSISTANT: The Integrated Sachs-Wolfe effect describes the fact that cosmic microwave background photons gain energy as they fall into potential wells and lose it again as they climb back out. The origin of the effect is gravitational redshift. For static potential wells, there is no net effect. However, if the potentials grow or decay, there is a net effect, known as the Integrated Sachs-Wolfe effect. In a matter-dominated universe, linear potentials stay constant. The late-time ISW effect refers to the fact that at late times, dark energy causes a deviation from matter domination and linear potentials decay. This causes an effect in the cosmic microwave background which has been measured in cross-correlation with large-scale structure. The early-time ISW refers to the fact that right after recombination, radiation still contributed a non-negligible amount of the energy density of the universe, causing potentials to grow. This causes the photons to gain energy.\n",
    "## EXAMPLE 2\n",
    "USER: What are the definitions of galaxy cluster mass (M200m, M500c, etc.)?\n",
    "ASSISTANT: Calculating a galaxy cluster's mass is like calculating the population of a sprawling city; as you get further out from city center, the suburbs never end. Eventually you have to stop counting and just say that from there on the people belong to a different town.\\nIn cluster masses, we integrate from the cluster center out to some radius rΔ where Δ might be 200, 500, or some other number. The definition of this radius is that the density of the enclosed mass is Δ times the average density of the universe at that redshift. The associated mass is MΔ.\\nOne more detail is that 'average density' can be defined in two ways. You can either include the energy density of dark energy - in which case you are comparing to the critical density at that redshift - or you can use the matter density only. This choice results in the c or m subscript (stands for critical or mean). Note that the critical density is higher than the mean density, so it gives an earlier cutoff in the integral. Therefore 'MΔc' masses are smaller than 'MΔm' ones.\n",
    "## QUESTION\n",
    "USER: {question}\n",
    "ASSISTANT:\"\"\"\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using the fine-tuned model as an assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_cosmosage(\"What is the temperature of the CMB?\") # easy question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_cosmosage(\"What is Digital Active Nulling?\") # see if it's read the arxiv paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_cosmosage(\"Explain the ISW effect.\")  # hard question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Push model to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# Upload all the content from the local folder to your remote Space.\n",
    "# By default, files are uploaded at the root of the repo\n",
    "api.upload_folder(\n",
    "    folder_path=\"/QUPMLcommon/tijmen/cosmosage_v0.4\",\n",
    "    repo_id=\"tijmen2/cosmosage_v0.4\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(path_or_fileobj=\"datasets/arxiv_qa2.jsonl\", path_in_repo=\"arxiv_qa.jsonl\", repo_id=\"Tijmen2/cosmology_qa\", repo_type=\"dataset\")\n",
    "# Upload all the content from the local folder to your remote Space.\n",
    "# By default, files are uploaded at the root of the repo\n",
    "# api.upload_folder(\n",
    "#     folder_path=\"/QUPMLcommon/tijmen/cosmosage_v0.3_gptq\",\n",
    "#     repo_id=\"tijmen2/cosmosage_v0.3_gptq\",\n",
    "#     repo_type=\"model\","
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALTERNATIVE STEP 2: Download the .tex files from arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # sequential version (one thread):\n",
    "# # scrape_arxiv.extract_tex(arxiv_ids, tex_files_path)\n",
    "\n",
    "# # multithreaded version:\n",
    "# from multiprocessing import Pool\n",
    "# def download_papers(arxiv_id_list):\n",
    "#     scrape_arxiv.extract_tex(arxiv_id_list, tex_files_path)\n",
    "# n_processes = 12\n",
    "# random.shuffle(arxiv_ids)\n",
    "# arxiv_id_split = [arxiv_ids[i::n_processes] for i in range(n_processes)]\n",
    "# with Pool(n_processes) as p:\n",
    "#     p.map(download_papers, arxiv_id_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALTERNATIVE Step 3: Parse the downloaded .tex files and save to JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # method using pydetex\n",
    "# # parsed_tex_files = tex_to_json.parse_tex_files(tex_files_path)\n",
    "# # tex_to_json.save_to_json(parsed_tex_files, json_file_path)\n",
    "\n",
    "# # method using command line detex \n",
    "# tex_to_json.detex_files(\"datasets/tex_files/\")\n",
    "# # manual regular expressions to clean up .detex and save to a single JSONL file\n",
    "# tex_to_json.detex_to_jsonl(\"datasets/tex_files/\", \"datasets/arxiv_tex.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
