{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import extract_textbooks\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the books into individual .txt files\n",
    "extract_textbooks.process_textbooks_multiprocess('datasets/astro_textbooks/')\n",
    "extract_textbooks.process_textbooks_multiprocess('datasets/physics_textbooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess, to get a list of books, where each book is a list of paragraphs\n",
    "book_paths = glob.glob('datasets/textbooks_extracted/*.txt')\n",
    "books_paragraphs = [extract_textbooks.preprocess_text(file_path) for file_path in book_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark each paragraph as good or bad based on whether the rate of certain\n",
    "# characters is within the distribution\n",
    "%matplotlib widget\n",
    "extract_textbooks.histogram_percentages(books_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = {\n",
    "    'spaces': (6, 24),\n",
    "    'digits': (0, 15),\n",
    "    'capital_letters': (1, 23),\n",
    "    'lowercase_letters': (50, 95),\n",
    "    'newlines': (0, 5),\n",
    "    'backslashes': (0, 5),\n",
    "    'periods': (0, 8),\n",
    "    'exclamation_marks': (0, 5),\n",
    "    'question_marks': (0, 6)\n",
    "}\n",
    "\n",
    "# Filter the textbooks\n",
    "filtered_books_paragraphs = extract_textbooks.filter_textbooks(books_paragraphs, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to JSON: full, training, and evaluation sets\n",
    "import json\n",
    "import random\n",
    "\n",
    "root = '/home/tijmen/cosmosage/datasets/'\n",
    "train_file = root+'textbooks_train.jsonl'\n",
    "eval_file = root+'textbooks_eval.jsonl'\n",
    "full_file = root+'textbooks.jsonl'\n",
    "\n",
    "# Collect all paragraphs\n",
    "all_paragraphs = []\n",
    "for book in filtered_books_paragraphs:\n",
    "    for para in book:\n",
    "        all_paragraphs.append({'text': para})\n",
    "\n",
    "# Shuffle the paragraphs\n",
    "random.shuffle(all_paragraphs)\n",
    "\n",
    "# Splitting the paragraphs into training and evaluation sets\n",
    "with open(train_file, 'w', encoding='utf-8') as train_f, \\\n",
    "     open(eval_file, 'w', encoding='utf-8') as eval_f, \\\n",
    "     open(full_file, 'w', encoding='utf-8') as full_f:\n",
    "    \n",
    "    for para in all_paragraphs:\n",
    "        # Write to full dataset file\n",
    "        full_f.write(json.dumps(para) + '\\n')\n",
    "\n",
    "        # Randomly decide whether to write to training or evaluation set\n",
    "        if random.random() < 0.8:  # 80% chance to go into training data\n",
    "            train_f.write(json.dumps(para) + '\\n')\n",
    "        else:\n",
    "            eval_f.write(json.dumps(para) + '\\n')\n",
    "\n",
    "# Also make a flat JSONL that has one entry per book\n",
    "flat_file = root+'textbooks_flat.jsonl'\n",
    "with open(flat_file, 'w', encoding='utf-8') as flat_f:\n",
    "    for book in filtered_books_paragraphs:\n",
    "        flat_book = '\\n\\n'.join(book)\n",
    "        flat_f.write(json.dumps({'text': flat_book}) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps\n",
    "# 1. see why some of the samples are crazy long, see if we can cut them into paragraphs\n",
    "# 2. tokenize. pad or discard short samples, chunk long samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
