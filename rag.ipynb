{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8EKMit4WNDY8"
      },
      "outputs": [],
      "source": [
        "# grab data as langchain docs\n",
        "import pickle\n",
        "with open('datasets/arxiv_cache/1210.4967.pkl', 'rb') as f:\n",
        "    d = pickle.load(f)\n",
        "\n",
        "docs = d['pages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OmsXOf59Pmm-"
      },
      "outputs": [],
      "source": [
        "# split any documents that are too long\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "chunked_docs = splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ixmCdRzBQ5gu"
      },
      "outputs": [],
      "source": [
        "# create the database\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L-ggaa763VRo"
      },
      "outputs": [],
      "source": [
        "# load model, tokenizer, retriever\n",
        "import torch\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "model_dir = \"models/cosmosage_v2/\"\n",
        "#model = AutoGPTQForCausalLM.from_quantized(model_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={'k': 4})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cR0k1cRWz8Pm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-19 20:57:56.447459: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-02-19 20:57:56.475773: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-19 20:57:56.475803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-19 20:57:56.476542: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-19 20:57:56.481495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-19 20:57:57.024184: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    temperature=0.4,\n",
        "    repetition_penalty=1.01,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=1000,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "prompt_template = \"\"\"Answer the question based on your knowledge. Use the following context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "USER: {question}\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_rI3YNp9Xl4s"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W7F07fQLXusU"
      },
      "outputs": [],
      "source": [
        "question = \"How does DAN suppress the input impedance of the SQUID?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "GYh-HG1l0De5",
        "outputId": "549e0bdd-b186-4d16-e7fa-90b3865d6f83"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DAN (Digital Active Nulling) suppresses the input impedance of the SQUID by actively nulling the current waveform at the input coil. This is achieved through a feedback loop where the SQUID's output is continuously compared to a reference voltage. Any deviation from the reference voltage is compensated by generating a nulling current that is inverted 180 degrees and fed back into the input coil. This nulling current effectively cancels out the input current, reducing the input impedance of the SQUID. The DAN technique is particularly useful in applications where high sensitivity and fast response times are required, such as in MRI systems and superconducting quantum interference devices (SQUIDs) used for measuring magnetic fields.\n"
          ]
        }
      ],
      "source": [
        "result = llm_chain.invoke({\"context\":\"\", \"question\": question})\n",
        "print(result['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "FZpNA3o10H10",
        "outputId": "9ddc0eef-0503-445d-8f70-26be3ec19de6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " DAN suppresses the input impedance of the SQUID by providing a nulling signal that actively zeroes the SQUID current across the bolometer bandwidth. This nulling signal is injected digitally and helps to improve the stability and linearity of the SQUID.\n"
          ]
        }
      ],
      "source": [
        "result_rag = rag_chain.invoke(question)\n",
        "print(result_rag['text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
