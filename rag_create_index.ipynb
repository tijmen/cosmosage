{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG: Create an index\n",
        "\n",
        "Let's use langchain, FAISS, and an embedding model to prepare an index. This will be used in the RAG pipeline at `rag_inference.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start by\n",
        " - reading in all the .mmd files\n",
        " - split into <=512 character chunks\n",
        " - write out a flattened list of langchain doc (text chunks) as pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EKMit4WNDY8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# chunking code in a helper function to be used with Pool\n",
        "def process_document(doc_path):\n",
        "    doc = TextLoader(doc_path).load()\n",
        "    chunked_doc = splitter.split_documents(doc)\n",
        "    return chunked_doc\n",
        "\n",
        "cache_file = \"datasets/mmd_cache.pkl\"\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "\n",
        "if os.path.exists(cache_file):\n",
        "    with open(cache_file, 'rb') as f:\n",
        "        d = pickle.load(f)\n",
        "    chunked_docs = d['chunked_docs']\n",
        "else:\n",
        "    filenames = glob.glob(\"datasets/*mmd/*.mmd\")\n",
        "    with Pool() as pool:\n",
        "        chunked_docs_list = pool.map(process_document, filenames)\n",
        "    chunked_docs = [chunk for sublist in chunked_docs_list for chunk in sublist] # flatten\n",
        "    d = {'filenames': filenames, 'chunked_docs': chunked_docs}\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(d, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating the index can be quite slow. If you're ok with doing it locally, uncomment the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixmCdRzBQ5gu"
      },
      "outputs": [],
      "source": [
        "# # alternatively, run it all locally:\n",
        "# import os\n",
        "# from langchain.vectorstores import FAISS\n",
        "# from langchain.embeddings import HuggingFaceEmbeddings\n",
        "# db_cache_file = \"datasets/faiss_index.bin\"\n",
        "# if os.path.exists(db_cache_file):\n",
        "#     # Load the existing FAISS index\n",
        "#     db = FAISS.load_local(db_cache_file)\n",
        "# else:\n",
        "#     db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5', multi_process=True))\n",
        "#     db.save_local(db_cache_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otherwise, let's offload this to a GPU cluster.\n",
        "\n",
        "Let's start by splitting the list of chunks into 8 equal parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = [chunked_docs[i::8] for i in range(8)]\n",
        "for i, chunk in enumerate(chunks):\n",
        "    with open(f\"datasets/mmd_cache_chunk{i}.pkl\", 'wb') as f:\n",
        "        pickle.dump(chunk, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "calculate the embeddings with up to 8 GPUs in parallel using a script like:\n",
        "\n",
        "```\n",
        "import os\n",
        "import argparse \n",
        "import pickle\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Create a FAISS index from a chunk of the MMD cache\")\n",
        "parser.add_argument(\"chunk\", type=int, help=\"The chunk number (0-7)\")\n",
        "args = parser.parse_args()\n",
        "chunk = args.chunk\n",
        "\n",
        "in_file = f\"input/mmd_cache_chunk/mmd_cache_chunk{chunk}.pkl\"\n",
        "out_file = f\"output/mmd_cache_chunk/faiss_index_chunk{chunk}.bin\"\n",
        "\n",
        "with open(in_file, \"rb\") as f:\n",
        "    chunked_docs = pickle.load(f)\n",
        "index = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5'))\n",
        "index.save_local(out_file)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have these, let's recombine into a single index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5')\n",
        "indices = [FAISS.load_local(f\"datasets/faiss_index_chunk{j}.bin\", embeddings) for j in range(8)]\n",
        "for i, thisindex in enumerate(indices):\n",
        "    if i == 0:\n",
        "        db = thisindex\n",
        "    else:\n",
        "        db.merge_from(thisindex)\n",
        "db.save_local(\"datasets/faiss_index.bin\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
