{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG: Create an index\n",
        "\n",
        "Let's use langchain, FAISS, and an embedding model to prepare an index. This will be used in the RAG pipeline at `rag_inference.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Start by\n",
        " - reading in all the .mmd files\n",
        " - split into <=512 character chunks\n",
        " - write out a flattened list of langchain doc (text chunks) as pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8EKMit4WNDY8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import glob\n",
        "import os\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# chunking code in a helper function to be used with Pool\n",
        "def process_document(doc_path):\n",
        "    doc = TextLoader(doc_path).load()\n",
        "    chunked_doc = splitter.split_documents(doc)\n",
        "    return [chunk for chunk in chunked_doc if len(chunk.page_content) > 100]\n",
        "\n",
        "cache_file = \"datasets/mmd_cache_arxiv_long.pkl\"\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)\n",
        "\n",
        "if os.path.exists(cache_file):\n",
        "    with open(cache_file, 'rb') as f:\n",
        "        d = pickle.load(f)\n",
        "    chunked_docs = d['chunked_docs']\n",
        "else:\n",
        "    filenames = glob.glob(\"datasets/arxiv_mmd/*.mmd\")\n",
        "    with Pool() as pool:\n",
        "        chunked_docs_list = pool.map(process_document, filenames)\n",
        "    chunked_docs = [chunk for sublist in chunked_docs_list for chunk in sublist] # flatten\n",
        "    d = {'filenames': filenames, 'chunked_docs': chunked_docs}\n",
        "    with open(cache_file, 'wb') as f:\n",
        "        pickle.dump(d, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generating the index can be quite slow. If you're ok with doing it locally, uncomment the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixmCdRzBQ5gu"
      },
      "outputs": [],
      "source": [
        "# alternatively, run it all locally:\n",
        "import os\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "db_cache_file = \"datasets/faiss_index_arxiv_long\"\n",
        "if os.path.exists(db_cache_file):\n",
        "    # Load the existing FAISS index\n",
        "    db = FAISS.load_local(db_cache_file)\n",
        "else:\n",
        "    db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5', multi_process=True))\n",
        "    db.save_local(db_cache_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otherwise, let's offload this to a GPU cluster.\n",
        "\n",
        "Let's start by splitting the list of chunks into 8 equal parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = [chunked_docs[i::8] for i in range(8)]\n",
        "for i, chunk in enumerate(chunks):\n",
        "    with open(f\"datasets/mmd_cache_chunk{i}.pkl\", 'wb') as f:\n",
        "        pickle.dump(chunk, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "calculate the embeddings with up to 8 GPUs in parallel using a script like:\n",
        "\n",
        "```\n",
        "import os\n",
        "import argparse \n",
        "import pickle\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Create a FAISS index from a chunk of the MMD cache\")\n",
        "parser.add_argument(\"chunk\", type=int, help=\"The chunk number (0-7)\")\n",
        "args = parser.parse_args()\n",
        "chunk = args.chunk\n",
        "\n",
        "in_file = f\"input/mmd_cache_chunk/mmd_cache_chunk{chunk}.pkl\"\n",
        "out_file = f\"output/mmd_cache_chunk/faiss_index_chunk{chunk}.bin\"\n",
        "\n",
        "with open(in_file, \"rb\") as f:\n",
        "    chunked_docs = pickle.load(f)\n",
        "index = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5'))\n",
        "index.save_local(out_file)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have these, let's recombine into a single index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5')\n",
        "indices = [FAISS.load_local(f\"datasets/faiss_index_chunk{j}.bin\", embeddings) for j in range(8)]\n",
        "for i, thisindex in enumerate(indices):\n",
        "    if i == 0:\n",
        "        db = thisindex\n",
        "    else:\n",
        "        db.merge_from(thisindex)\n",
        "db.save_local(\"datasets/faiss_index.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Edit database\n",
        "\n",
        "In RAG I've found that the database contains many standalone titles which are often retrieved. Let's filter out short chunks as these are likely to be titles or other kinds of non-useful text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name='BAAI/bge-large-en-v1.5')\n",
        "index = FAISS.load_local(\"datasets/faiss_index.bin\", embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_community.docstore.in_memory.InMemoryDocstore at 0x147607987cd0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index.docstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not str",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/faiss_index.bin/index.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     10\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 11\u001b[0m chunk_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchunk_lengths\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Adjust the key according to your metadata structure\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 3: Create a histogram of chunk lengths\u001b[39;00m\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(chunk_lengths, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)  \u001b[38;5;66;03m# Adjust bins as needed\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the FAISS index\n",
        "index = faiss.read_index('datasets/faiss_index.bin/index.faiss')\n",
        "\n",
        "# Step 2: Load the metadata\n",
        "with open('datasets/faiss_index.bin/index.pkl', 'rb') as f:\n",
        "    metadata = pickle.load(f)\n",
        "chunk_lengths = metadata['chunk_lengths']  # Adjust the key according to your metadata structure\n",
        "\n",
        "# Step 3: Create a histogram of chunk lengths\n",
        "plt.hist(chunk_lengths, bins=30)  # Adjust bins as needed\n",
        "plt.title('Histogram of Chunk Lengths')\n",
        "plt.xlabel('Chunk Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Filter vectors with chunk length > 100 and save to a new FAISS index\n",
        "long_chunks_indices = [i for i, length in enumerate(chunk_lengths) if length > 100]\n",
        "\n",
        "d = index.d  # Dimension of the vectors\n",
        "reduced_index = faiss.IndexFlatL2(d)  # Or use the same type as your original index\n",
        "\n",
        "# Assuming sequential access, this is efficient for Flat indexes but may be slow for large, complex indexes\n",
        "for i in long_chunks_indices:\n",
        "    vector = faiss.vector_to_array(index.reconstruct(i)).reshape(1, -1)  # Retrieve vector\n",
        "    reduced_index.add(vector)  # Add vector to the new index\n",
        "\n",
        "# Save the reduced index to disk\n",
        "faiss.write_index(reduced_index, 'reduced_index.faiss')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = metadata[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'InMemoryDocstore' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'InMemoryDocstore' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "ds[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
